package com.wtf;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class SecondMapper extends Mapper<Object,Text,IntWritable,IntWritable>{
	public void map(Object key,Text values,Context context) throws IOException,InterruptedException{
		StringTokenizer stObj = new StringTokenizer(values.toString().substring(2));
		Integer usr 1= Integer.parseInt(values.toString().substring(0,1));
		while(stObj.hasMoreTokens()){
			Integer usr1=Integer.parseInt(stObj.nextToken().toString().trim());
			StringTokenizer stObjCopy = new StringTokenizer(values.toString().substring(2));
			while(stObjCopy.hasMoreTokens()){
				Integer usr2=Integer.parseInt(stObjCopy.nextToken().toString().trim());
				if((!(usr1==usr2))&&(!(usr1<0||usr2<0))){
					context.write(new IntWritable(usr1),new IntWritable(usr2));	
				}
			}
			if(usr1<0)
				context.write(new IntWritable(usr), new IntWritable(usr1));
		}
	}
}
